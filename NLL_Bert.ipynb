{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "import random\n",
    "seed(1)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def gaussian_nll(ytrue, ypreds):\n",
    "    \"\"\"Keras implmementation of multivariate Gaussian negative loglikelihood loss function. \n",
    "    This implementation implies diagonal covariance matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ytrue: tf.tensor of shape [n_samples, n_dims]\n",
    "        ground truth values\n",
    "    ypreds: tf.tensor of shape [n_samples, n_dims*2]\n",
    "        predicted mu and logsigma values (e.g. by your neural network)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    neg_log_likelihood: float\n",
    "        negative loglikelihood averaged over samples\n",
    "        \n",
    "    This loss can then be used as a target loss for any keras model, e.g.:\n",
    "        model.compile(loss=gaussian_nll, optimizer='Adam') \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_dims = int(int(ypreds.shape[1])/2)\n",
    "    mu = ypreds[:, 0:n_dims]\n",
    "    logsigma = ypreds[:, n_dims:]\n",
    "    \n",
    "    mse = -0.5*K.sum(K.square((ytrue-mu)/K.exp(logsigma)),axis=1)\n",
    "    sigma_trace = -K.sum(logsigma, axis=1)\n",
    "    log2pi = -0.5*n_dims*np.log(2*np.pi)\n",
    "    \n",
    "    log_likelihood = mse+sigma_trace+log2pi\n",
    "\n",
    "    return K.mean(-log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval_data = \"/Users/zxj/Desktop/study/semester3/MCS/humor/data/task-1/data/task-1/test_eval.csv\"\n",
    "\n",
    "test_eval=pd.read_csv(test_eval_data)\n",
    "test_eval_label=test_eval.meanGrade\n",
    "\n",
    "#get orignial news\n",
    "test_eval_news=test_eval.original\n",
    "test_eval_funny_word=test_eval.edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords=set(stopwords.words('english'))\n",
    "tt=nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "punctuation = '!,;:?\"\\'.\\'/<>'\n",
    "def removePunctuation(text):\n",
    "    text = re.sub(r'[{}]+'.format(punctuation),'',text)\n",
    "    return text.strip()\n",
    "\n",
    "#catch the changed word\n",
    "def find_tag(news):\n",
    "    p = re.compile(r'[<](.*?)/[>]', re.S)\n",
    "    return re.findall(p, news)\n",
    "\n",
    "def preprocess_news(sentence,funny_word):\n",
    "    final=[]\n",
    "    num=0\n",
    "    bad_sign=[\"’\",\"‘\",\"-\"]\n",
    "    \n",
    "    for event in sentence:\n",
    "        lines=[]       \n",
    "        event=str(event)\n",
    "        changed_word=find_tag(event)   \n",
    "        event=removePunctuation(event)\n",
    "        event=tt.tokenize(event)\n",
    "        #tokenize and remove Punctuation\n",
    "        \n",
    "        for i in event:\n",
    "            if i not in bad_sign:\n",
    "                \n",
    "                if i in changed_word:\n",
    "                    #add the funny behind the changed word\n",
    "                    #lines.append(i)\n",
    "                    lines.append(funny_word[num])\n",
    "                else:\n",
    "                    lines.append(i)\n",
    "                    \n",
    "        lines=' '.join(lines)\n",
    "        lines='[CLS]'+' '+lines+' '+'[SEP]'\n",
    "        final.append(lines)\n",
    "        num+=1\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval_news=preprocess_news(test_eval_news,test_eval_funny_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] The Latest Election tally shows Cars turning right [SEP]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eval_news[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def negative_binomial_layer(x):\n",
    "    \"\"\"\n",
    "    Lambda function for generating negative binomial parameters\n",
    "    n and p from a Dense(2) output.\n",
    "    Assumes tensorflow 2 backend.\n",
    "    \n",
    "    Usage\n",
    "    -----\n",
    "    outputs = Dense(2)(final_layer)\n",
    "    distribution_outputs = Lambda(negative_binomial_layer)(outputs)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tf.Tensor\n",
    "        output tensor of Dense layer\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    out_tensor : tf.Tensor\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the number of dimensions of the input\n",
    "    num_dims = len(x.get_shape())\n",
    "    \n",
    "    # Separate the parameters\n",
    "    n, p = tf.unstack(x, num=2, axis=-1)\n",
    "    \n",
    "    # Add one dimension to make the right shape\n",
    "    n = tf.expand_dims(n, -1)\n",
    "    p = tf.expand_dims(p, -1)\n",
    "        \n",
    "    # Apply a softplus to make positive\n",
    "    n = tf.keras.activations.softplus(n)\n",
    "    \n",
    "    # Apply a sigmoid activation to bound between 0 and 1\n",
    "    p = tf.keras.activations.sigmoid(p)\n",
    "\n",
    "    # Join back together again\n",
    "    out_tensor = tf.concat((n, p), axis=num_dims-1)\n",
    "\n",
    "    return out_tensor\n",
    "\n",
    "def negative_binomial_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Negative binomial loss function.\n",
    "    Assumes tensorflow backend.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf.Tensor\n",
    "        Ground truth values of predicted variable.\n",
    "    y_pred : tf.Tensor\n",
    "        n and p values of predicted distribution.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    nll : tf.Tensor\n",
    "        Negative log likelihood.\n",
    "    \"\"\"\n",
    "\n",
    "    # Separate the parameters\n",
    "    n, p = tf.unstack(y_pred, num=2, axis=-1)\n",
    "    \n",
    "    # Add one dimension to make the right shape\n",
    "    n = tf.expand_dims(n, -1)\n",
    "    p = tf.expand_dims(p, -1)\n",
    "    \n",
    "    # Calculate the negative log likelihood\n",
    "    nll = (\n",
    "        tf.math.lgamma(n) \n",
    "        + tf.math.lgamma(y_true + 1)\n",
    "        - tf.math.lgamma(n + y_true)\n",
    "        - n * tf.math.log(p)\n",
    "        - y_true * tf.math.log(1 - p)\n",
    "    )                  \n",
    "\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer, TFAlbertModel\n",
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2',token_ids_1=None)\n",
    "inputs_train = tokenizer(test_eval_news,padding=True,return_tensors=\"pt\")\n",
    "inputs_train=np.array(inputs_train['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing TFAlbertModel: ['predictions']\n",
      "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 37)]              0         \n",
      "_________________________________________________________________\n",
      "tf_albert_model_2 (TFAlbertM TFBaseModelOutputWithPool 11683584  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1538      \n",
      "=================================================================\n",
      "Total params: 11,685,122\n",
      "Trainable params: 11,685,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer=tf.keras.Input(shape=(37,),dtype='int64')\n",
    "bert=TFAlbertModel.from_pretrained('albert-base-v2', return_dict=True)(input_layer)\n",
    "layer1=bert[1]\n",
    "regression=tf.keras.layers.Dense(2,activation='linear')(layer1)\n",
    "#result=Lambda(negative_binomial_layer)(regression)\n",
    "model=tf.keras.Model(inputs=input_layer,outputs=regression)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 37) for input Tensor(\"input_3:0\", shape=(None, 37), dtype=int64), but it was called on an input with incompatible shape (None, 36).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 37) for input Tensor(\"input_3:0\", shape=(None, 37), dtype=int64), but it was called on an input with incompatible shape (None, 36).\n",
      "31/31 [==============================] - 478s 15s/step - loss: 15.7936\n",
      "Epoch 2/4\n",
      "31/31 [==============================] - 485s 16s/step - loss: 0.9554\n",
      "Epoch 3/4\n",
      "31/31 [==============================] - 495s 16s/step - loss: 1.0672\n",
      "Epoch 4/4\n",
      "31/31 [==============================] - 507s 16s/step - loss: 0.8912\n"
     ]
    }
   ],
   "source": [
    "opt=tf.keras.optimizers.Adam(learning_rate=3e-4, epsilon=1e-08, clipnorm=1.0)\n",
    "metric = tf.keras.metrics.MeanSquaredError()\n",
    "model.compile(optimizer=opt, loss=gaussian_nll)\n",
    "model_fit = model.fit(inputs_train, test_eval_label, \n",
    "                      batch_size=100, epochs=4\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob(model, x, batch_size=2048):\n",
    "    \"\"\"Make predictions given model and 2d data\n",
    "    \"\"\"\n",
    "\n",
    "    ypred = model.predict(x, batch_size=batch_size, verbose=1)\n",
    "    n_outs = int(ypred.shape[1] / 2)\n",
    "    mean = ypred[:, 0:n_outs]\n",
    "    sigma = np.exp(ypred[:, n_outs:])\n",
    "\n",
    "    return mean, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33027509093914764\n"
     ]
    }
   ],
   "source": [
    "print(np.var(test_eval_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1.1050736],\n",
       "        [1.1050733],\n",
       "        [1.1050735],\n",
       "        [1.1050735],\n",
       "        [1.1050736],\n",
       "        [1.1050733],\n",
       "        [1.1050733],\n",
       "        [1.1050735],\n",
       "        [1.1050733]], dtype=float32), array([[0.60179824],\n",
       "        [0.60179836],\n",
       "        [0.60179824],\n",
       "        [0.6017982 ],\n",
       "        [0.6017983 ],\n",
       "        [0.60179806],\n",
       "        [0.6017982 ],\n",
       "        [0.60179824],\n",
       "        [0.601798  ]], dtype=float32))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_prob(model,inputs_train[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2     2    21    78   949   898  2062  5077  7192  5310    26   469\n",
      "    13 11881    16  4778 13458   140   132   436  1894   119  1158 10527\n",
      "     3     3     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(inputs_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 37) for input Tensor(\"input_5:0\", shape=(None, 37), dtype=int64), but it was called on an input with incompatible shape (None, 36).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-6.428763 ],\n",
       "        [-6.4259925],\n",
       "        [-6.4203596],\n",
       "        [-6.4225764],\n",
       "        [-6.4274507],\n",
       "        [-6.4285426]], dtype=float32), array([[0.12186965],\n",
       "        [0.12236995],\n",
       "        [0.12289025],\n",
       "        [0.12279273],\n",
       "        [0.12233145],\n",
       "        [0.12194616]], dtype=float32)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(inputs_train[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
