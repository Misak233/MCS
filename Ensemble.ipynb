{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval_data = \"/Users/zxj/Desktop/study/semester3/MCS/humor/data/task-1/data/task-1/test_eval.csv\"\n",
    "\n",
    "test_eval=pd.read_csv(test_eval_data)\n",
    "test_eval_label=test_eval.meanGrade\n",
    "\n",
    "#get orignial news\n",
    "test_eval_news=test_eval.original\n",
    "test_eval_funny_word=test_eval.edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords=set(stopwords.words('english'))\n",
    "tt=nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "punctuation = '!,;:?\"\\'.\\'/<>'\n",
    "def removePunctuation(text):\n",
    "    text = re.sub(r'[{}]+'.format(punctuation),'',text)\n",
    "    return text.strip()\n",
    "\n",
    "#catch the changed word\n",
    "def find_tag(news):\n",
    "    p = re.compile(r'[<](.*?)/[>]', re.S)\n",
    "    return re.findall(p, news)\n",
    "\n",
    "def preprocess_news(sentence,funny_word):\n",
    "    final=[]\n",
    "    num=0\n",
    "    bad_sign=[\"’\",\"‘\",\"-\"]\n",
    "    \n",
    "    for event in sentence:\n",
    "        lines=[]       \n",
    "        event=str(event)\n",
    "        changed_word=find_tag(event)   \n",
    "        event=removePunctuation(event)\n",
    "        event=tt.tokenize(event)\n",
    "        #tokenize and remove Punctuation\n",
    "        \n",
    "        for i in event:\n",
    "            if i not in bad_sign:\n",
    "                \n",
    "                if i in changed_word:\n",
    "                    #add the funny behind the changed word\n",
    "                    #lines.append(i)\n",
    "                    lines.append(funny_word[num])\n",
    "                else:\n",
    "                    lines.append(i)\n",
    "                    \n",
    "        lines=' '.join(lines)\n",
    "        lines='[CLS]'+' '+lines+' '+'[SEP]'\n",
    "        final.append(lines)\n",
    "        num+=1\n",
    "    return final\n",
    "\n",
    "test_eval_news=preprocess_news(test_eval_news,test_eval_funny_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer, TFAlbertModel\n",
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def import_model(filename):\n",
    "    tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2',token_ids_1=None)\n",
    "    inputs_train = tokenizer(test_eval_news,padding=True,return_tensors=\"pt\")\n",
    "    inputs_train=np.array(inputs_train['input_ids'])\n",
    "    input_layer=tf.keras.Input(shape=(37,),dtype='int64')\n",
    "    bert=TFAlbertModel.from_pretrained('albert-base-v2', return_dict=True)(input_layer)\n",
    "    layer1=bert[1]\n",
    "    classfier=tf.keras.layers.Dense(1,activation='sigmoid')(layer1)\n",
    "    model=tf.keras.Model(inputs=input_layer,outputs=classfier)\n",
    "    #model.summary()\n",
    "    model.load_weights(filename)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing TFAlbertModel: ['predictions']\n",
      "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing TFAlbertModel: ['predictions']\n",
      "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing TFAlbertModel: ['predictions']\n",
      "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing TFAlbertModel: ['predictions']\n",
      "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing TFAlbertModel: ['predictions']\n",
      "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model1=import_model('./checkpoints/bert_model6')\n",
    "model2=import_model('./checkpoints/bert_model5')\n",
    "model3=import_model('./checkpoints/bert_model4')\n",
    "model4=import_model('./checkpoints/bert_model3')\n",
    "model5=import_model('./checkpoints/bert_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
